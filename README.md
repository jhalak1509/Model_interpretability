# Model_interpretability

This assignment encompassed the analysis of three predictive models—linear regression, random forest, and an AutoML stacked ensemble—applied to the prediction of flight delays. The linear regression model elucidated feature coefficients, while the tree-based model provided insights into decision nodes and feature importance. AutoML, employing a diverse ensemble, showcased a unified approach. SHAP analysis was then employed to interpret and compare the models, revealing variations in feature importance. Notably, time emerged as pivotal in linear regression and random forest, while AutoML emphasized the significance of airline and time. This exercise highlighted the nuanced perspectives each model brings to predicting flight delays, emphasizing the importance of interpretability in understanding their decision-making processes.
