# Model Interpretability

1. Fit a linear model and interpret the regression coefficients

2. Fit a tree-based model and interpret the nodes

3.  Use auto ml to find the best model

4. Run SHAP analysis on the models from steps 1, 2, and 3,   interpret the SHAP values and compare them with the other model interpretability methods.

This assignment encompassed the analysis of three predictive models—linear regression, random forest, and an AutoML stacked ensemble—applied to the prediction of flight delays. The linear regression model elucidated feature coefficients, while the tree-based model provided insights into decision nodes and feature importance. AutoML, employing a diverse ensemble, showcased a unified approach. SHAP analysis was then employed to interpret and compare the models, revealing variations in feature importance. Notably, time emerged as pivotal in linear regression and random forest, while AutoML emphasized the significance of airline and time. This exercise highlighted the nuanced perspectives each model brings to predicting flight delays, emphasizing the importance of interpretability in understanding their decision-making processes.
